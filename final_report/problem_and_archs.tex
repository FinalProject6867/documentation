% !TEX root = main.tex

\section{Problem Statement and Architectures}
\label{sec:problem_and_archs}

\rhnote{Define input and output. Say using cnn. describe software. Describe one-shot learning. then say we investigated the following changes}
\rhnote{Currently, the input format is a 32x32x1 depth map and a 1x7 pose vector. }
\rhnote{For the following results, we use the balanced dataset with our input as the image for each data point and the 7-dimesional grasp vector. 
The data was split into 80\%, 10\% and 10\% for the training, validation and testing sets respectively.}
Use keras ~\cite{chollet2017keras}

\subsection{Balancing Data Sets}
\label{sec:balance}

As mentioned previously, Dex-Net 2.0 contains approximately 20\% positive examples. 

\rhnote{Show what happens if we just use the 80/20 data with confusion matrix. Therefore for the rest of this we used balanced data set. Obviously this limits us because cant use all data set, so We sample}

\subsection{Data Set Size}
\label{sec:size}

\rhnote{To achieve desired ratio mentioned above and for computational reasons, we sample the size of points we use. We can vary this. 10k vs 50k} 

\subsection{Network Structure}
\label{sec:archs}

\rhnote{Describe each of the archs we use. Update this from before}

Andreas Network ~\cite{viereck2017learning}

Below we describe and show the results of two achitectures, which we refer to as the Inception Network~\cite{szegedy2015going} and the ResNet.

The inception network consists of 1 convolution layer in the beginning with 10 filters of size 3x3. 
The output of this layer is passed in parallel to three convolutional layers of sizes 1x1, 3x3, and 5x5, each with 16 filters. 
These outputs are concatenated on the depth dimension and passed through a max pooling layer of 3x3. 
The outputs are flattened with global average pooling and then the pose vector is concatenated before passed to a classifier of one hidden layer of 20 units, as seen in \figref{fig:inception_net}. 

The residual network consists of two convolutional layers, one with 8 filters of size 7x7 and the next with 16 filters of size 3x3.
At this point, the output of this layer branches, such that this same output is passed through two more convolution layers of 32 filters 3x3 and 16 filters 1x1 used as dimension reduction. 
The output of these two layers is added to their input and then passed to another convolution layer of 8 filters of 1x1 for further dimension reduction and then flattened with global average pooling. 
Like for the other network, the pose vector is concatenated to this output before passing it to a classifier with a fully connected layer of 10 hidden units, as shown in \figref{fig:res_net}.

\subsection{Normalization}
\label{sec:normalization}

\rhnote{use l1 and l2 regularization for fully connected layers. use drop out for conv. doesnt make as much sense to use batch normalization because network is not very large}