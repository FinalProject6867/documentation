% !TEX root = main.tex

\section{Related Work}
\label{sec:related_work}

\rhnote{pull all papers from bibtex and folder and give summaries}
\rhnote{below are notes}

BigBird is "a high-quality, large-scale dataset of 3D object instances, with accurate calibration information for every image. We anticipate that “solving” this dataset will effectively remove many perception-related problems for mobile, sensing-based robots."~\cite{singh2014bigbird} 

Grasp detection algorithm for cluttered environments. Generate grasp hypothesis, develop description. Sample a bunch of these and then do binary classification task using a 4-layer CNN. Use big bird~\cite{pas2017grasp}

In this paper, we take the leap of increasing the available training data to 40 times more than prior work, leading to a dataset size of 50K data points collected over 700 hours of robot grasping attempts. This allows us to train a Convolutional Neural Network (CNN) for the task of predicting grasp locations without severe overfitting. In our formulation, we recast the regression problem to an 18-way binary classification over image patches (model grasp-able from different angles)~\cite{pinto2016supersizing}

Detect grasps from RGBD image. Two layer network, early deep learning grasping paper, deals with rgbd instead of just rgb image~\cite{lenz2015deep}. 

grasp proposals from images (not deep learning). focused on novel objects. more descriptive grasp representation, use SVMs \cite{jiang2011efficient}

estimate grasp robustness from local contact patch (use deep learning and random forest)~\cite{seita2016large}

we use dex net 2.0 which builds off of dex net 1.0. This paper presents the Dexterity Network (DexNet) 1.0, a dataset of 3D object models and a sampling-based planning algorithm to explore how Cloud Robotics can be used for robust grasp planning. The algorithm uses a MultiArmed Bandit model with correlated rewards to leverage prior grasps and 3D object models. Dex-Net 1.0 uses Multi-View Convolutional Neural Networks (MV-CNNs), a new deep learning method for 3D object classification, to provide a similarity metric between objects~\cite{mahler2016dex}

present a grasp stability predictor that uses spatio-temporal tactile features collected from the early-object-lifting phase to predict the grasp outcome with a high accuracy. then used to learn grasp readjustments. \cite{chebotar2016self}

action condition video prediction over pixel movement to learn physical interactions, use on a dataset of 59,000 robot interactions involving pushing motions,\cite{finn2016unsupervised}

Deducing whether a particular grasp will be successful from indirect measurements, such as vision, is therefore quite challenging, and direct sensing of contacts through touch sensing provides an appealing avenue toward more successful and consistent robotic grasping. However, in order to fully evaluate the value of touch sensing for grasp outcome prediction, we must understand how touch sensing can influence outcome prediction accuracy when combined with other modalities. Build  visuo-tactile deep neural network models to directly predict grasp outcomes from either modality individually, and from both modalities together. \cite{calandra2017feeling}

learning-based approach to hand-eye coordination for robotic grasping from monocular images. To learn hand-eye coordination for grasping, we trained a large convolutional neural network to predict the probability that taskspace motion of the gripper will result in successful grasps, using only monocular camera images independent of camera calibration or the current robot pose~\cite{levine2016learning}

Dex Net 3.0 uses suction instead, with CNN\cite{mahler2017suction}. 

Hard to collect real robot data. Want to use simulation but simulation doesnt model world precisely. We study how randomized simulated environments and domain adaptation methods can be extended to train a grasping system to grasp novel objects from raw monocular RGB images. (improve accuracy with good simulation and first to use monocular rgb)~\cite{bousmalis2017using}


This paper takes advantage of new development in non-linear optimization and formulates the problem of grasp synthesis as a single constrained optimization problem, generating grasps that are at the same time feasible for the hand’s kinematics and optimal according to a force related quality measure.~\cite{el2012bridging}

This paper presents a new method for paralleljaw grasping of isolated objects from depth images, under large gripper pose uncertainty. Whilst most approaches aim to predict the single best grasp pose from an image, our method first predicts a score for every possible grasp pose, which we denote the grasp function. To learn this function, we train a Convolutional Neural Network which takes as input a single depth image of an object, and outputs a score for each grasp pose across the image. Training data for this is generated by use of physics simulation and depth image simulation with 3D object meshes. rhnote{Use same structure as alexnet}~\cite{johns2016deep}

novel approach to multi-fingered grasp planning leveraging learned deep neural network models. We train a convolutional neural network to predict grasp success as a function of both visual information of an object and grasp configuration. We can then formulate grasp planning as inferring the grasp configuration which maximizes the probability of grasp success~\cite{luplanning}

This paper presents a method for one-shot learning of dexterous grasps and grasp generation for novel objects. A model of each grasp type is learned from a single kinesthetic demonstration and several types are taught. These models are used to select and generate grasps for unfamiliar objects. Both the learning and generation stages use an incomplete point cloud from a depth camera. model uses kernel-density estimation. ~\cite{kopicki2016one}

deep learning architecture
for detecting the palm and fingertip positions of stable grasps
directly from partial object views. multi-finger. model is learning grasp quality metrics. Deep Model with local contrast normalization (LCN), 5 convolutional layers, and 3 fully connected layers. experiment with different parameters~\cite{varley2015generating}

s paper considers learning grasps in the full 6D position and orientation pose space for non-parallel-jaw grippers. We generate a database of millions of simulated successful and unsuccessful grasps for a three-fingered underactuated gripper and thousands of objects, and then learn a modified convolutional neural network (CNN) to predict grasp quality from overhead depth images of novel objects.\cite{zhou20176dof}

i
