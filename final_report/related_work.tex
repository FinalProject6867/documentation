% !TEX root = main.tex

\section{Related Work}
\label{sec:related_work}

While we primarily build off of the Dex Net 2.0 paper~\cite{mahler2017dex}, there is a wide range of literature investigating learning how to grasp objects. 
The overwhelming majority of the recent work has focused on using CNNs, although there are a few papers that use SVMs, kernel-density estimation and constrained optimization-based techniques~\cite{jiang2011efficient,kopicki2016one,el2012bridging}. 

Ten Pas et al~\cite{pas2017grasp} developed a grasp detection algorithm similar to the Dex Net paper by generating grasp hypotheses and training a 4-layer CNN to perform binary classification on whether the grasp is viable. 
They use a different grasp representation and rely on the BigBird data set~\cite{singh2014bigbird}. 
Rather then classifying a grasp, Johns et al uses a CNN to learn a grasp function, which provides a score for each grasp~\cite{johns2016deep}. 
At execution time, then can compare the scores of several grasps and select the best grasp.

The above works focus on using a parallel jaw gripper, a two finger hand where the fingers are parallel to each other and usually move together. 
While this is a relatively simple hand, it is ubiquitous in industry and research and still allows for complex manipulation tasks~\cite{mason2011generality}. 
However, people have worked to expand this grasp prediction to more complex, multi-fingered hands using various CNN architectures~\cite{luplanning,varley2015generating,zhou20176dof}. 

Within the grasp learning community, and in fact, within the robotics learning community, there is a pull between real data collected through a robotic platform and data generated from a physics simulator. 
While data collected on a robot better captures reality (since physics simulators are far from perfect), data collection is difficult and time-consuming. 
Pinto et al collected, at the time, a record amount of data at 50k data points of grasps collected across 700 robot hours~\cite{pinto2016supersizing}.
Levine et al later collected 800,000 grasp attempts over a two month period, using between six and fourteen robot arms at once~\cite{levine2016learning}. 
While these approaches allowed them to train a CNN without over fitting or using simulation data, such data collection is not always practical and require a huge amount of engineering effort. 
Bousmalis showed how to augment a smaller amount of real data with simulation to improve accuracy, thus attempting to combine the merits of both~\cite{bousmalis2017using}. 

While most of this work focuses on using color (RGB) or depth (RGBD) images~\cite{lenz2015deep}, there is growing interest in using tactile feedback, inspired by how humans feel as they grasp. 
Calandra et al combines vision and touch sensing to build a visuo-tactile CNN that predicts grasp outcomes from a combination of the modalities~\cite{calandra2017feeling}. 
This can go one step further in using tactile feedback to learn how to readjust while grasping~\cite{chebotar2016self}. 

Dex Net 2.0 is the second of three pieces of research. 
Dex Net 1.0 solves the same grasping problem, but uses a multi-armed bandit model to correlate the rewards of a proposed grasp with previously seen grasps~\cite{mahler2016dex}.
The similarity metric between grasps is learned from a Multi-View CNN. 
Dex Net 3.0 uses a CNN to learn suction points, leveraging recent interest in using suction for pick and place motions~\cite{mahler2017suction}. 

\begin{comment}
BigBird is "a high-quality, large-scale dataset of 3D object instances, with accurate calibration information for every image. We anticipate that “solving” this dataset will effectively remove many perception-related problems for mobile, sensing-based robots."~\cite{singh2014bigbird} 

Grasp detection algorithm for cluttered environments. Generate grasp hypothesis, develop description. Sample a bunch of these and then do binary classification task using a 4-layer CNN. Use big bird~\cite{pas2017grasp}

In this paper, we take the leap of increasing the available training data to 40 times more than prior work, leading to a dataset size of 50K data points collected over 700 hours of robot grasping attempts. This allows us to train a Convolutional Neural Network (CNN) for the task of predicting grasp locations without severe overfitting. In our formulation, we recast the regression problem to an 18-way binary classification over image patches (model grasp-able from different angles)~\cite{pinto2016supersizing}

Detect grasps from RGBD image. Two layer network, early deep learning grasping paper, deals with rgbd instead of just rgb image~\cite{lenz2015deep}. 

grasp proposals from images (not deep learning). focused on novel objects. more descriptive grasp representation, use SVMs \cite{jiang2011efficient}

estimate grasp robustness from local contact patch (use deep learning and random forest)~\cite{seita2016large}

we use dex net 2.0 which builds off of dex net 1.0. This paper presents the Dexterity Network (DexNet) 1.0, a dataset of 3D object models and a sampling-based planning algorithm to explore how Cloud Robotics can be used for robust grasp planning. The algorithm uses a MultiArmed Bandit model with correlated rewards to leverage prior grasps and 3D object models. Dex-Net 1.0 uses Multi-View Convolutional Neural Networks (MV-CNNs), a new deep learning method for 3D object classification, to provide a similarity metric between objects~\cite{mahler2016dex}

present a grasp stability predictor that uses spatio-temporal tactile features collected from the early-object-lifting phase to predict the grasp outcome with a high accuracy. then used to learn grasp readjustments. \cite{chebotar2016self}

action condition video prediction over pixel movement to learn physical interactions, use on a dataset of 59,000 robot interactions involving pushing motions,\cite{finn2016unsupervised}

Deducing whether a particular grasp will be successful from indirect measurements, such as vision, is therefore quite challenging, and direct sensing of contacts through touch sensing provides an appealing avenue toward more successful and consistent robotic grasping. However, in order to fully evaluate the value of touch sensing for grasp outcome prediction, we must understand how touch sensing can influence outcome prediction accuracy when combined with other modalities. Build  visuo-tactile deep neural network models to directly predict grasp outcomes from either modality individually, and from both modalities together. \cite{calandra2017feeling}

learning-based approach to hand-eye coordination for robotic grasping from monocular images. To learn hand-eye coordination for grasping, we trained a large convolutional neural network to predict the probability that taskspace motion of the gripper will result in successful grasps, using only monocular camera images independent of camera calibration or the current robot pose~\cite{levine2016learning}

Dex Net 3.0 uses suction instead, with CNN\cite{mahler2017suction}. 

Hard to collect real robot data. Want to use simulation but simulation doesnt model world precisely. We study how randomized simulated environments and domain adaptation methods can be extended to train a grasping system to grasp novel objects from raw monocular RGB images. (improve accuracy with good simulation and first to use monocular rgb)~\cite{bousmalis2017using}


This paper presents a new method for paralleljaw grasping of isolated objects from depth images, under large gripper pose uncertainty. Whilst most approaches aim to predict the single best grasp pose from an image, our method first predicts a score for every possible grasp pose, which we denote the grasp function. To learn this function, we train a Convolutional Neural Network which takes as input a single depth image of an object, and outputs a score for each grasp pose across the image. Training data for this is generated by use of physics simulation and depth image simulation with 3D object meshes. rhnote{Use same structure as alexnet}~\cite{johns2016deep}

novel approach to multi-fingered grasp planning leveraging learned deep neural network models. We train a convolutional neural network to predict grasp success as a function of both visual information of an object and grasp configuration. We can then formulate grasp planning as inferring the grasp configuration which maximizes the probability of grasp success~\cite{luplanning}

This paper presents a method for one-shot learning of dexterous grasps and grasp generation for novel objects. A model of each grasp type is learned from a single kinesthetic demonstration and several types are taught. These models are used to select and generate grasps for unfamiliar objects. Both the learning and generation stages use an incomplete point cloud from a depth camera. model uses kernel-density estimation. ~\cite{kopicki2016one}

deep learning architecture
for detecting the palm and fingertip positions of stable grasps
directly from partial object views. multi-finger. model is learning grasp quality metrics. Deep Model with local contrast normalization (LCN), 5 convolutional layers, and 3 fully connected layers. experiment with different parameters~\cite{varley2015generating}

s paper considers learning grasps in the full 6D position and orientation pose space for non-parallel-jaw grippers. We generate a database of millions of simulated successful and unsuccessful grasps for a three-fingered underactuated gripper and thousands of objects, and then learn a modified convolutional neural network (CNN) to predict grasp quality from overhead depth images of novel objects.\cite{zhou20176dof}
\end{comment}
