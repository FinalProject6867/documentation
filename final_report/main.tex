\documentclass[letterpaper, 10 pt, conference]{../ieeeconf} 
\IEEEoverridecommandlockouts
\overrideIEEEmargins
\pdfoptionpdfminorversion=4

\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{textcomp}
\usepackage{graphicx}
\usepackage[font=footnotesize]{subcaption}
\usepackage[font=footnotesize]{caption}
\usepackage{hyperref}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage[normalem]{ulem}
\usepackage{verbatim}
\usepackage[export]{adjustbox}
\usepackage{amsmath}
\usepackage{url}
\usepackage{siunitx}
\usepackage[utf8]{inputenc}
\usepackage[TS1,T1]{fontenc}
\usepackage{array, booktabs}
\usepackage{caption}
\usepackage[cal=cm]{mathalfa}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}

% Labels in IEEE format
\newcommand{\eref}[1]{(\ref{#1})} % Equation
\newcommand{\sref}[1]{Sec.~\ref{#1}} % Section
\newcommand{\figref}[1]{Fig.~\ref{#1}} % Figure
\newcommand{\tref}[1]{Table~\ref{#1}} % Table
\newcommand{\aref}[1]{Algorithm~\ref{#1}} % Algorithm
\newcommand{\lref}[1]{Line~\ref{#1}} % Line
\renewcommand*\rmdefault{ppl}
\setlength{\textfloatsep}{5pt}

\usepackage{ifthen}
\usepackage[usenames,dvipsnames,table]{xcolor}
\newboolean{include-notes}
\setboolean{include-notes}{true} 
% http://en.wikibooks.org/wiki/LaTeX/Colors
\newcommand{\rhnote}[1]{\ifthenelse{\boolean{include-notes}}%
 {\textcolor{blue}{\textbf{RH: #1}}}{}}
\newcommand{\sanote}[1]{\ifthenelse{\boolean{include-notes}}%
 {\textcolor{green}{\textbf{SAN: #1}}}{}}

\begin{document}

% paper title
\title{6.857 Final Project: Milestone 6}
\author{Sebastiani Aguirre Navarro and Rachel Holladay}
\maketitle

\input{intro.tex}
\input{related_work.tex}
\input{data_set.tex}

\section{Results}
\label{sec:results}
Use keras ~\cite{chollet2017keras}
Andreas Network ~\cite{viereck2017learning}


During these stages of development, we are only evaluating our results according to the validation set. 
Since we are not yet achieving good enough performance on the validation set and are not doing parameter selection, we do not yet need to evaulate on the testing set. 

For the following results, we use the balanced dataset with our input as the image for each data point and the 7-dimesional grasp vector. 
During training, 80\% of the dataset was used as train set and the remaining 20\% as test set. 
Below we describe and show the results of two achitectures, which we refer to as the Inception Network~\cite{szegedy2015going} and the ResNet.
As discussed in \sref{sec:questions}, these are the some of the many networks we will be testing.  

The inception network consists of 1 convolution layer in the beginning with 10 filters of size 3x3. 
The output of this layer is passed in parallel to three convolutional layers of sizes 1x1, 3x3, and 5x5, each with 16 filters. 
These outputs are concatenated on the depth dimension and passed through a max pooling layer of 3x3. 
The outputs are flattened with global average pooling and then the pose vector is concatenated before passed to a classifier of one hidden layer of 20 units, as seen in \figref{fig:inception_net}. 

The residual network consists of two convolutional layers, one with 8 filters of size 7x7 and the next with 16 filters of size 3x3.
At this point, the output of this layer branches, such that this same output is passed through two more convolution layers of 32 filters 3x3 and 16 filters 1x1 used as dimension reduction. 
The output of these two layers is added to their input and then passed to another convolution layer of 8 filters of 1x1 for further dimension reduction and then flattened with global average pooling. 
Like for the other network, the pose vector is concatenated to this output before passing it to a classifier with a fully connected layer of 10 hidden units, as shown in \figref{fig:res_net}.
 
In \figref{fig:inception_results}, we can see that for the inception netowrk, the training loss decreases while the validation loss, while oscilating, increases. 
The same trend is shown in \figref{fig:resnet_results} for the residual net. 
This means that, like the results in our previous milestone, the network is overfitting to the data. 
The final training accuracy for inception net and residual net are 70\% and 75\% respectively, while both do 50\% on the validation set. 
One possible modification is to add regularization on the fully connected layers, or to modify the architectures by removing or adding more layers. 
We will continue to explore this as well as experimenting with the representation of the data.

    



\begin{comment}
\begin{figure}[t!]
    \centering
        \includegraphics[width=0.7\columnwidth]{figs/inception_net.png}
    \caption{Network Structure of Inception Net} \label{fig:inception_net}
\end{figure}
\end{comment}

\section{Research Questions}
\label{sec:questions}
Below we present several of the research questions we will continue to explore. 

\subsection{Input Format}
One of the benefits of the Dex-Net data set is that it provides many features, allowing us to vary what we use as input to the network. 
We are and will continue to experiment with various inputs. 
Some inputs might be particularly informative but costly to collect when running the system on a real robot. 
Currently, the input format is a 32x32x1 depth map and a 1x7 pose vector. 
It is our intention to also use the representation used in the DexNet paper as soon as it becomes available~\footnote{We are coordinating with the authors of the paper}.
This representation is as described in the Data Set section and shall serve as our baseline. 

\subsection{Balancing Data Sets}
As mentioned previously, Dex-Net 2.0 contains approximately 20\% positive examples. 
This is not inherenely problematic given that the training and testing sets are drawn from the same distribution, with this same ratio.
However, by sampling subsets of our data set, we can achieve any positive-to-negative ratio and thus explore how changing this ratio effects accuracy. 
By doing this, we avoid a model to overfit and artificially think it is doing well by predicting one class on all samples.
(For example, a very trival way to achieve 80\% accuracy would be to predict negative for all examples.)

\subsection{Data Set Size}
The Dex-net data set contains 6.7 million data points. 
For computational reasons, we are sampling a subset of these points. 
However, we can vary the size of this subset to compare the trade-off between the accuracy and the size of the training set. 
Deep learning models tend to require a rather large quantity of data depending the classification task that it is being learned and how complex the data is.

For now, since the input data consists of just small 32x32x1 images and 1x7 vectors, and we learning a binary classification task, we reasoned that we could start by using a small subset of the 6.7 million data points to produce a model of reasonable performance.

\subsection{Architecture Structure}
One of the largest sources of experimentation thus far and continuing forward is our choice of architecture. 
As our starting point, we produced an architecture that follows the concept of residual networks. 
This model consists of instead of learning a direct mapping of the input to the output, to learn ``residue'' over this input. Subsequently, we produced another architecture that follows the Inception model.
Both were discribed in \sref{sec:results} and have batch normalization after each convolution, to mitigate internal covariances shifts, as well as a dropout rate of 0.7. 
This are initial architectures and not final. 
As we keep training and modifying them accordingly, we will come up with the proper parameters using the validation and testing accuracy as our metric.

\subsection{Normalization}
As seen in our results, we are falling prone to overfitting. 
Therefore, it is critical to explore how we can regularize the system. 

{\footnotesize
    \bibliographystyle{ieeetr}
\bibliography{../references}}

\end{document}
