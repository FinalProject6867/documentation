% !TEX root = main.tex

\section{Data Set}
\label{sec:data_set}

We opted to use the Dex Net 2.0 data set due to its size, ease of use and parameterization~\cite{mahler2017dex}. 
Large published grasping data sets are rare within robotics, both because the field (data based learning for manipulation) is new and because such data sets are generally difficult to collect. 

Mahler et al define a generative graphical model defined over the camera pose, object shape and pose, friction coefficient, grasp, depth image and success metric. 
To generate the data set they make i.i.d (independent and identically distributed) samples from their generative graphical model, resulting in 6.7 million data points. 

The data set is defined over 1,500 object meshes that were used in Dex-Net 1.0~\cite{mahler2016dex}, collected from a variety of other data bases and standardized with respect to position.
For each object, they generated 100 parallel jaw grasps via rejection sampling of antipodal pairs and evaluated a robust epsilon quality grasp metric on each grasp~\cite{seita2016large}. 
Additionally, each object is paired with a rendered depth image (2.5D point cloud\footnote{The images are 2D matrices that are referred to as 2.5D in robotics literature because they display depth information.}) from the sampled camera pose. 

The data set of 6.7 million data points has 21.1\% positive examples. 
This is unsurprising, since it is much more difficult to find successful grasps, as compared to failed grasps. 

From the data set we randomly sample, with replacement, $k$ data points. 
In some cases we sample such that we guarantee some ratio of positive versus negative examples to explore the effect of varying distributions.  