\documentclass[letterpaper, 10 pt]{article} 

\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{graphicx}
\usepackage[font=footnotesize]{subcaption}
\usepackage[font=footnotesize]{caption}
\usepackage{hyperref}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage[normalem]{ulem}
\usepackage{verbatim}
\usepackage[export]{adjustbox}
\usepackage{amsmath}
\usepackage{url}
\usepackage{siunitx}
\usepackage[utf8]{inputenc}
\usepackage[TS1,T1]{fontenc}
\usepackage{array, booktabs}
\usepackage{caption}
\usepackage[cal=cm]{mathalfa}

% Labels in IEEE format
\newcommand{\eref}[1]{(\ref{#1})} % Equation
\newcommand{\sref}[1]{Sec.~\ref{#1}} % Section
\newcommand{\figref}[1]{Fig.~\ref{#1}} % Figure
\newcommand{\tref}[1]{Table~\ref{#1}} %Table
\newcommand{\aref}[1]{Algorithm~\ref{#1}} %Algorithm
\renewcommand*\rmdefault{ppl}
\setlength{\textfloatsep}{5pt}

\usepackage{ifthen}
\usepackage[usenames,dvipsnames,table]{xcolor}
\newboolean{include-notes}
\setboolean{include-notes}{true} 
% http://en.wikibooks.org/wiki/LaTeX/Colors
\newcommand{\rhnote}[1]{\ifthenelse{\boolean{include-notes}}%
 {\textcolor{blue}{\textbf{RH: #1}}}{}}

\begin{document}

% paper title
\title{6.857 Final Project: Milestone 1}
\author{Laura Jarin-Lipshitz, Sebastiani Aguirre Navarro and Rachel Holladay}
\maketitle

Reliable grasping is at the heart of most robotic manipulation tasks. 
The goal of our project is predict grasp success and quality to enable reliable grasping. 
We imagine a scene with a robotic manipulator, a goal object to be grasped, and camera providing a view of the goal object.
We want to decide how to grasp that goal object such that maximizes our probability of a successful grasp.
Generating grasps given a view of the scene is its own difficult research area~\cite{pinto2016supersizing,lenz2015deep,jiang2011efficient}.
Therefore, for the context of this machine learning-focused project, we will assume we have a black box way to generate grasps (detailed later). 
Therefore our learning input is an image and proposed grasp and our desired output is a metric of success of quality. 
By learning this relation, we could score and then rank all possible grasps for an object and execute the best one. 

The relatively recent interest in the intersection of large scale learning and manipulation has led to the publication of several grasping datasets. 
We have investigated and are (currently) planning to use both DexNet 2.0~\cite{mahler2017dex} and BigBIRD~\cite{singh2014bigbird}. 
\begin{enumerate}
   \item DexNet 2.0 provides 6.7 million sets of synthetic point clouds, grasps and analytical grasp metrics. Here, the grasps are provided by the data set, eliminating the need to generate them. The grasps are for a parallel jaw-gripper and the dataset is based off of millions of 3D models of common objects on a table.  
   \item BigBIRD is a dataset across 125 objects placed on a table, where each object has 600 images and 3D point clouds. This creates a dataset of 75,000 sets. While this is a smaller set then DexNet, the point clouds are not synthetic. ten Pas et al utilized this dataset to generate grasps and classify their success rate~\cite{pas2017grasp}. Their grasp generation method for a parallel jaw-gripper is open source, allowing us to use it as a black box. 
\end{enumerate}

Our hope is that we can utilize both the datasets, nearly interchangably, to compare across sythetic and real point cloud data and to compare the effect of the size of the dataset. 
Following the lead of following papers, we plan on create and applying a CNN architecture. 
We will not be using the learning code released with either project. 
We plan to evaluate our approach based on our performance on a hold-out set of test data. 
Due to time constraints, we do not expect to evaluate our prediction method on a real robotic platform. 




{\footnotesize
    \bibliographystyle{ieeetr}
\bibliography{../references}}

\end{document}
