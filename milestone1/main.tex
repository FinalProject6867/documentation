\documentclass[letterpaper, 10 pt]{article} 

\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{graphicx}
\usepackage[font=footnotesize]{subcaption}
\usepackage[font=footnotesize]{caption}
\usepackage{hyperref}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage[normalem]{ulem}
\usepackage{verbatim}
\usepackage[export]{adjustbox}
\usepackage{amsmath}
\usepackage{url}
\usepackage{siunitx}
\usepackage[utf8]{inputenc}
\usepackage[TS1,T1]{fontenc}
\usepackage{array, booktabs}
\usepackage{caption}
\usepackage[cal=cm]{mathalfa}

% Labels in IEEE format
\newcommand{\eref}[1]{(\ref{#1})} % Equation
\newcommand{\sref}[1]{Sec.~\ref{#1}} % Section
\newcommand{\figref}[1]{Fig.~\ref{#1}} % Figure
\newcommand{\tref}[1]{Table~\ref{#1}} %Table
\newcommand{\aref}[1]{Algorithm~\ref{#1}} %Algorithm
\renewcommand*\rmdefault{ppl}
\setlength{\textfloatsep}{5pt}

\usepackage{ifthen}
\usepackage[usenames,dvipsnames,table]{xcolor}
\newboolean{include-notes}
\setboolean{include-notes}{true} 
% http://en.wikibooks.org/wiki/LaTeX/Colors
\newcommand{\rhnote}[1]{\ifthenelse{\boolean{include-notes}}%
 {\textcolor{blue}{\textbf{RH: #1}}}{}}

\begin{document}

% paper title
\title{6.857 Final Project: Milestone 1}
\author{Laura Jarin-Lipschitz, Sebastiani Aguirre Navarro and Rachel Holladay}
\maketitle

Reliable grasping is at the heart of most robotic manipulation tasks. 
The goal of our project is predict grasp success to enable reliable grasping. 
We imagine a scene with a robotic manipulator, a goal object to be grasped, and a camera providing a view of the goal object.
We want to decide how to grasp that goal object such that our probability of a successful grasp is maximized.
Generating grasps given a view of the scene is its own difficult research area~\cite{pinto2016supersizing,lenz2015deep,jiang2011efficient}.
Therefore, for the context of this machine learning-focused project, we will assume we have a black box way to generate grasps drawn from existing, open source packages, as detailed in the dataset description below.
Therefore, our learning input is an image and proposed grasp, and our desired output is a metric of probability of grasp success. 
By learning this relation, we will score and then rank all possible grasps for an object, allowing execution of the best grasp. 

The relatively recent interest in the intersection of large scale learning and manipulation has led to the publication of several recent grasping datasets. 
We have investigated and currently plan to use both DexNet 2.0~\cite{mahler2017dex} and BigBIRD~\cite{singh2014bigbird}. 
\begin{enumerate}
   \item DexNet 2.0 provides 6.7 million sets of synthetic point clouds, grasps and analytical grasp metrics. Here, the grasps are provided by the data set, eliminating the need to generate them. The grasps are for a parallel jaw-gripper and the dataset is based off of millions of 3D models of common objects on a table.  
   \item BigBIRD is a dataset across 125 objects placed on a table, where each object has 600 images and 3D point clouds. This creates a dataset of 75,000 samples. While this is a smaller set then DexNet, the point clouds are not synthetic. ten Pas et al utilized this dataset to generate grasps and classify their success rate~\cite{pas2017grasp}. Their grasp generation method for a parallel jaw-gripper is open source, allowing us to use it as a black box. 
\end{enumerate}

Our hope is that we can utilize both the datasets, nearly interchangeably with respect to our architecture, to compare across synthetic and real point cloud data and to compare the effect of the size of the dataset on the quality of results. 
Following the methods of the cited papers, we plan to create and apply a CNN architecture, but we will not replicate either architecture from the literature. 
We plan to evaluate our approach based on our performance on a hold-out set of test data. 
Due to time constraints, we do not expect to evaluate our prediction method on a physical robotic platform, and instead will use a model-based robotics simulator such as Gazebo. 




{\footnotesize
    \bibliographystyle{ieeetr}
\bibliography{../references}}

\end{document}
