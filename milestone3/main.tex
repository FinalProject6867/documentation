\documentclass[letterpaper, 10 pt]{article} 

\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{graphicx}
\usepackage[font=footnotesize]{subcaption}
\usepackage[font=footnotesize]{caption}
\usepackage{hyperref}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage[normalem]{ulem}
\usepackage{verbatim}
\usepackage[export]{adjustbox}
\usepackage{amsmath}
\usepackage{url}
\usepackage{siunitx}
\usepackage[utf8]{inputenc}
\usepackage[TS1,T1]{fontenc}
\usepackage{array, booktabs}
\usepackage{caption}
\usepackage[cal=cm]{mathalfa}

% Labels in IEEE format
\newcommand{\eref}[1]{(\ref{#1})} % Equation
\newcommand{\sref}[1]{Sec.~\ref{#1}} % Section
\newcommand{\figref}[1]{Fig.~\ref{#1}} % Figure
\newcommand{\tref}[1]{Table~\ref{#1}} %Table
\newcommand{\aref}[1]{Algorithm~\ref{#1}} %Algorithm
\renewcommand*\rmdefault{ppl}
\setlength{\textfloatsep}{5pt}

\usepackage{ifthen}
\usepackage[usenames,dvipsnames,table]{xcolor}
\newboolean{include-notes}
\setboolean{include-notes}{true} 
% http://en.wikibooks.org/wiki/LaTeX/Colors
\newcommand{\rhnote}[1]{\ifthenelse{\boolean{include-notes}}%
 {\textcolor{blue}{\textbf{RH: #1}}}{}}
\newcommand{\sanote}[1]{\ifthenelse{\boolean{include-notes}}%
 {\textcolor{green}{\textbf{SAN: #1}}}{}}


\begin{document}

% paper title
\title{6.857 Final Project: Milestone 3}
\author{Sebastiani Aguirre Navarro and Rachel Holladay}
\maketitle

\section{Software Infrastructure}
%For milestone 3, describe the software infrastucture you will use: whatâ€™s in a library versus what are you coding from scratch. How is the work split amongst your team? What questions are open?

\rhnote{Edit what is commented below to only talk about software. Need to look into data set in more detail to describe}
Our goal is to use machine learning techniques to predict the probability of successfully grasping an unknown object with a robotic arm. 
The infrastructure of our project is broken into two sections: the data generation and the learning. 

Our data will come as point clouds from the Big Bird dataset~\cite{singh2014bigbird}, which provides depth maps, RGB-D images and meshes.
We have already secured access to this dataset, and while there are 125 objects in the data set, we do not expect to use all of them. 
The next step in the data generation process is creating labeled grasps from the point cloud. 
We will be using the open source grasp generation and labeling process package "Grasp Pose Generator (GPG)" from~\cite{pas2017grasp}. 
The package detect 6 degree of freedom poses for parallel jaw grippers from point clouds. 
The code is available through a Github repository and has a few dependencies, such as ROS and Eigen.
Our goal is to use this package to generate data as soon as possible. 

Given our labeled data, when can then turn to our learning algorithms. 
We will use Tensorflow and Keras to implement our learning algorithms. Keras specifically containes predefined many of the common layers used in deep neural networks.
This will prove useful to run initial prototypes that build upon these common layers. Eventually, we will have to tweak the layers or add layers that are not predefined in Keras.
This is where we will turn to Tensorflow. While Tensorflow already has many basic predefined layers, prototyping in it can be tedious. However Tensorflow provides the fundamental tools for defining custom operations and layers.
There is also an existing ``Model Zoo'' for Keras and Tensorflow. Since many of the proposed architectures have been already trained with the purpose of detecting or classifying objects, one strategy is to properly modify the input layers and
perform Transfer Learning to see if we can reduce the training time significantly in our grasper network when tuning it on the RGBD data. We will also write from scratch several prototypes that apply the concepts of ResNet, Network in Network
and Inception so that we can compare it to other transfered off the shelf models. 

\section{Open Questions}

\rhnote{I dont really understand how this is different from last time..?}  
\sanote{I guess, on a more technical level, an open question would be what kind of data representation is best for the learning..., aside that IDK}

%One major risk is that our main goal of improving the architecture of the CNN is not achieved. We hope to treat the network in the paper as the baseline and, by exploring other architectures, outperform the network. however there is a risk that we will not find an architecture with better performance, especially given the difference in our evaluation methods from theirs. However, even if we cannot improve on the percent grasp success in simulation, we hope to explore the effects of different architectures on the types of objects that the network fails to predict correctly. 

%Another risk we will is our choice to evaluate our CNN's performance in simulation. We made this choice due to time and resource limitations. However, robotics is inherently about a real robot interacting with the physical world. Therefore, there is a part of the project (from the robotics side, not the ML side) that is incomplete, since simulated grasps are highly dependent on the fidelity of the robot model.


\section{Division of Labor}

While our group was originally started with three people, one of our group members has dropped the course, forcing us to change our division of labor. 
Rachel will generate the data set from the open source repositories and set up the labeling mechanism. 
Sebastiani will set up an environment in a cloud provider with GPUs and start coding the Network In Network and Inception based models in Keras/Tensorflow and transfer learn layers of a pretrained YOLO onto our problem space.  

{\footnotesize
    \bibliographystyle{ieeetr}
\bibliography{../references}}

\end{document}
